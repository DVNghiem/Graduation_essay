{"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JmfvkNr_rMLG","executionInfo":{"status":"ok","timestamp":1652924013241,"user_tz":-420,"elapsed":16659,"user":{"displayName":"Văn Nghiêm","userId":"03593021723549294443"}},"outputId":"dfb9bc66-f946-4987-eeca-a5e2d363b680"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":14,"status":"ok","timestamp":1652924013242,"user":{"displayName":"Văn Nghiêm","userId":"03593021723549294443"},"user_tz":-420},"id":"ggWfVQDqE9_u","colab":{"base_uri":"https://localhost:8080/"},"outputId":"46d6b824-931a-49a0-a8d4-8c5525be05ca"},"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/.shortcut-targets-by-id/1T-TkLmhLIA8qodciANOIZEcC0W8CsU9V/Khóa luận tốt nghiệp\n"]}],"source":["%cd /content/drive/MyDrive/Khóa luận tốt nghiệp"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HdEqM6srFF9Y","executionInfo":{"status":"ok","timestamp":1652924091667,"user_tz":-420,"elapsed":78432,"user":{"displayName":"Văn Nghiêm","userId":"03593021723549294443"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"ef3dbac5-ab1e-41da-ee88-7680b90d1ed7"},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[K     |████████████████████████████████| 63.9 MB 1.4 MB/s \n","\u001b[K     |████████████████████████████████| 4.2 MB 40.5 MB/s \n","\u001b[K     |████████████████████████████████| 462 kB 53.9 MB/s \n","\u001b[K     |████████████████████████████████| 84 kB 3.0 MB/s \n","\u001b[K     |████████████████████████████████| 596 kB 31.3 MB/s \n","\u001b[K     |████████████████████████████████| 6.6 MB 40.8 MB/s \n","\u001b[K     |████████████████████████████████| 2.6 MB 7.2 MB/s \n","\u001b[?25h  Building wheel for vncorenlp (setup.py) ... \u001b[?25l\u001b[?25hdone\n","\u001b[K     |████████████████████████████████| 68 kB 4.4 MB/s \n","\u001b[?25h  Building wheel for fasttext (setup.py) ... \u001b[?25l\u001b[?25hdone\n","\u001b[K     |████████████████████████████████| 8.5 MB 7.6 MB/s \n","\u001b[K     |████████████████████████████████| 965 kB 58.8 MB/s \n","\u001b[?25h"]}],"source":["!pip install genz-tokenize -q \n","!pip install vncorenlp -q \n","!pip install fasttext -q \n","!pip install pyvi -q"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_j7-SUnIFIwz"},"outputs":[],"source":["from genz_tokenize import Tokenize, get_embedding_matrix\n","import pandas as pd\n","import numpy as np\n","import string\n","from sklearn.model_selection import train_test_split\n","import matplotlib.pyplot as plt\n","import fasttext\n","import tensorflow as tf\n","import pickle\n","from vncorenlp import VnCoreNLP\n","from pyvi import ViTokenizer, ViPosTagger"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DmHyXigfFRkS"},"outputs":[],"source":["data = pd.read_csv('/content/drive/MyDrive/Khóa luận tốt nghiệp/weight_of_word/data2.xlsx', header = None)\n","data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Zmo11hWJFWat"},"outputs":[],"source":["x = data.values[:, 0]\n","y = data.values[:, 1]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yJncd3QFFYHK"},"outputs":[],"source":["punc = string.punctuation\n","punc = punc.replace('_', '')\n","punc = punc.replace('/', '')\n","punc = punc.replace(',', '')\n","punc = punc.replace('$', '')\n","punc = punc.replace('%', '')\n","def preprocess(text):\n","    text = [i for i in text if i not in punc]\n","    text = ''.join(text)\n","    return text.lower()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HRiMSiAKFZjb"},"outputs":[],"source":["x = [preprocess(i) for i in x]\n","y = [preprocess(i) for i in y]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hML12UV8FbOz"},"outputs":[],"source":["tokenizer = Tokenize()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YdDPpDyLFrvK"},"outputs":[],"source":["# pickle.dump(embedding_matrix, open('weight_of_word/emb.pkl', 'wb'))\n","embedding_matrix = get_embedding_matrix()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"34efriJzGSMz"},"outputs":[],"source":["maxlen = max(277, max([len(i.split()) for i in x]))\n","maxlen"]},{"cell_type":"code","source":["def vncore_tokenize(text):\n","    result = vncore.tokenize(text)\n","    text = ' '\n","    for i in result:\n","        text+= ' '.join(i)+' '\n","    return text.strip()"],"metadata":{"id":"p5CCEt60Mfoc"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oJxgdpKSGbEI"},"outputs":[],"source":["train_y = []\n","for i, j in zip(x, y):\n","    s = []\n","    for c in i.split():\n","        if c in j:\n","            a = np.random.uniform(0.8, 0.95)\n","        else:\n","            a = np.random.uniform(0.1, 0.3)\n","        s.append(a)\n","    s = s+[0]*(maxlen - len(s))\n","    train_y.append(s)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yrRo7jJwGpyu"},"outputs":[],"source":["train_x, test_x, train_y, test_y = train_test_split(x, train_y, test_size = 0.1)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3Q6QXc-iGt6l"},"outputs":[],"source":["label = ['ai', 'cai gi', 'con vat', 'nhu the nao', 'number', 'tai sao', 'thoi gian', 'time', 'thuc vat', 'yes no', 'location']\n","class Embedding(tf.keras.layers.Layer):\n","    def __init__(self, vocab_size, dim,max_position_embeddings, **kwargs):\n","        super().__init__(**kwargs)\n","        self.vocab_size = vocab_size\n","        self.dim = dim\n","        self.max_position_embeddings = max_position_embeddings\n","        self.LayerNorm = tf.keras.layers.LayerNormalization(epsilon=1e-12, name=\"LayerNorm\")\n","        self.dropout = tf.keras.layers.Dropout(rate=0.1)\n","\n","    def build(self, input_shape: tf.TensorShape):\n","        with tf.name_scope(\"word_embeddings\"):\n","            self.weight = self.add_weight(\n","                name=\"weight\",\n","                shape=[self.vocab_size, self.dim],\n","            )\n","\n","        with tf.name_scope(\"position_embeddings\"):\n","            self.position_embeddings = self.add_weight(\n","                name=\"embeddings\",\n","                shape=[self.max_position_embeddings, self.dim],\n","            )\n","        super().build(input_shape)\n","\n","    def call(self, input_ids=None):\n","        inputs_embeds = tf.gather(params=self.weight, indices=input_ids)\n","        input_shape = tf.shape(input_ids)\n","        position_ids = tf.expand_dims(tf.range(start=0, limit=input_shape[-1]), axis=0)\n","        position_embeds = tf.gather(params=self.position_embeddings, indices=position_ids)\n","        final_embeddings = inputs_embeds + position_embeds\n","        final_embeddings = self.LayerNorm(inputs=final_embeddings)\n","        final_embeddings = self.dropout(inputs=final_embeddings)\n","        return final_embeddings\n","\n","class FeedFoward(tf.keras.layers.Layer):\n","    def __init__(self, d_model, num_class):\n","        super(FeedFoward, self).__init__()\n","        self.flatten = tf.keras.layers.Flatten()\n","        self.fc1 = tf.keras.layers.Dense(d_model)\n","        self.dropout = tf.keras.layers.Dropout(0.2)\n","        self.out = tf.keras.layers.Dense(num_class, activation = 'softmax')\n","    \n","    def call(self, inputs):\n","        inputs = self.flatten(inputs)\n","        x = self.fc1(inputs)\n","        x = self.dropout(x)\n","        return self.out(x)\n","class ClassifyModel(tf.keras.Model):\n","    def __init__(self, vocab_size, max_position_embeddings, d_model, num_heads, num_class):\n","        super(ClassifyModel, self).__init__()\n","        self.embedding = Embedding(vocab_size, d_model, max_position_embeddings)\n","        self.layernorm = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n","        self.mha = tf.keras.layers.MultiHeadAttention(num_heads, d_model, dropout = 0.1)\n","        self.fc = tf.keras.layers.Dense(1)\n","        self.fw = FeedFoward(d_model, num_class)\n","    def call(self, inputs):\n","        x = self.embedding(inputs)\n","        mask = self.mask(inputs)\n","        out_mha = self.mha(x, x, x)\n","        x = self.fc(self.layernorm(x+out_mha))\n","        return self.fw(x)\n","\n","    def mask(self, inputs):\n","        masks = tf.logical_not(tf.math.equal(inputs, 0))\n","        masks = tf.cast(masks, dtype = tf.float32)\n","        return masks[:, tf.newaxis, tf.newaxis, :]\n","\n","model_cls = ClassifyModel(tokenizer.vocab_size(), 1000, 256, 3, len(label))"]},{"cell_type":"code","source":["checkpoint = tf.train.Checkpoint(\n","        model=model_cls)\n","ckpt_manager = tf.train.CheckpointManager(\n","    checkpoint, '/content/drive/MyDrive/Khóa luận tốt nghiệp/Question Classify/Model_Ver1/checpoint', max_to_keep=5)\n","if ckpt_manager.latest_checkpoint:\n","    checkpoint.restore(ckpt_manager.latest_checkpoint)\n","    print('\\nLatest checkpoint restored!!!\\n')\n"],"metadata":{"id":"2LzKPdk8iQ3M"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DJvr0AD-F01b"},"outputs":[],"source":["postag_label = ['B', 'Np', 'Nc', 'Nu', 'N', 'Ny', 'Ni', 'Nb', 'V', 'Vb', 'A', 'Ab', 'P', 'R', 'L', 'M', 'E', 'C', 'Cc', 'I', 'T', 'Y', 'Z', 'X', 'CH']\n","len(postag_label)"]},{"cell_type":"code","source":["vncore_jar = '/content/drive/MyDrive/Khóa luận tốt nghiệp/Paragraph_Raking/Tensorflow/VnCoreNLP/VnCoreNLP-1.1.1.jar'\n","vncore = VnCoreNLP(vncore_jar)"],"metadata":{"id":"xHzwUUyQz5NE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def pos_tag(text, vncore):\n","    text = text.replace('_', ' ')\n","    tag = vncore.pos_tag(text)\n","    combine = []\n","    for i in tag:\n","        combine+=i\n","    return combine"],"metadata":{"id":"B4r4cIIVznh8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def getTypeOfWord(text):\n","    out = np.zeros(shape = (maxlen, len(postag_label)))\n","    pos = pos_tag(text, vncore)\n","    for i in range(len(pos)):\n","        if i == 277:\n","            print(pos)\n","        index_pos = postag_label.index(pos[i][1])\n","        out[i, index_pos] = 1\n","    return out"],"metadata":{"id":"_0Z-Ccr_ijBe"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"F2VuUKHwBuNx"},"outputs":[],"source":["class Dataset(tf.keras.utils.Sequence):\n","    def __init__(self, train_x, train_y, batch_size, maxlen, postag_label):\n","        self.train_x = train_x\n","        self.train_y = train_y\n","        self.size = len(train_x)\n","        self.batch_size = batch_size\n","        self.maxlen = maxlen\n","        self.postag_label = postag_label\n","        self.indices = np.arange(len(self.train_x))\n","    def __len__(self):\n","        return self.size//self.batch_size\n","    \n","    def __getitem__(self, idx):\n","        inds = self.indices[idx * self.batch_size:(idx + 1) * self.batch_size]\n","        x1 = np.array([tokenizer(self.train_x[i], \n","                                 max_len = self.maxlen, \n","                                 padding = True, \n","                                 truncation = True)['input_ids'] for i in inds])\n","        x2 = np.array([getTypeOfWord(self.train_x[i]) for i in inds])\n","        y = np.array([self.train_y[i] for i in inds]).astype('float32')\n","        return x1, x2, y\n","    def on_epoch_end(self):\n","        np.random.shuffle(self.indices)\n","    \n","\n","dataset_train = Dataset(train_x, train_y, 32, maxlen, postag_label)\n","dataset_test = Dataset(test_x, test_y, 32, maxlen, postag_label)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BJMwal5jD9_n"},"outputs":[],"source":["for x1, x2, y in dataset_train:\n","    break\n","x1.shape, x2.shape, y.shape\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GgPainQQ-S6W"},"outputs":[],"source":["def fullConnected(d_model):\n","    model = tf.keras.Sequential()\n","    model.add(tf.keras.layers.Dense(d_model*2, activation='gelu'))\n","    model.add(tf.keras.layers.Dense(d_model, activation = 'sigmoid'))\n","    return model\n","\n","class QuestionAnalys(tf.keras.Model):\n","    def __init__(self, d_model, maxlen, model_cls, num_heads):\n","        super(QuestionAnalys, self).__init__()\n","        self.model_cls = model_cls\n","        self.model_cls.trainable = False\n","\n","        self.mha1 = tf.keras.layers.MultiHeadAttention(num_heads, d_model)\n","        self.mha2 = tf.keras.layers.MultiHeadAttention(num_heads, d_model)\n","\n","        self.layerNorm1 = tf.keras.layers.LayerNormalization()\n","        self.layerNorm2 = tf.keras.layers.LayerNormalization()\n","\n","        self.fc1 = fullConnected(d_model)\n","        self.fc2 = fullConnected(d_model)\n","        self.fc3 = fullConnected(d_model)\n","\n","        self.fc_postag = tf.keras.layers.Dense(d_model)\n","        self.fc_type = tf.keras.layers.Dense(d_model)\n","\n","        self.out1 = tf.keras.layers.Dense(1, activation = 'gelu')\n","        self.flatten = tf.keras.layers.Flatten()\n","        self.out2 = tf.keras.layers.Dense(d_model, activation = 'gelu')\n","        self.out3 = tf.keras.layers.Dense(maxlen)\n","\n","        self.dropout1 = tf.keras.layers.Dropout(0.2)\n","        self.dropout2= tf.keras.layers.Dropout(0.2)\n","\n","    def mask(self, inputs):\n","        masks = tf.logical_not(tf.math.equal(inputs, 0))\n","        masks = tf.cast(masks, dtype = tf.float32)\n","        return masks[:, tf.newaxis, tf.newaxis, :]\n","    \n","    def call(self, x):\n","        inputs, input_postag = x\n","        emb = self.model_cls.embedding(inputs)\n","        mask = self.mask(inputs)\n","        out_mha = self.mha1(emb, emb, emb, attention_mask = mask)\n","        norm = self.layerNorm1(out_mha+emb)\n","        ff = self.fc1(self.dropout1(norm))\n","\n","        postag = self.fc_postag(input_postag)\n","        out_mha = self.mha2(ff, postag, ff, attention_mask = mask)\n","        norm = self.layerNorm2(ff+out_mha)\n","        ff = self.fc2(self.dropout2(norm))\n","\n","        inputs = tf.cast(inputs, dtype = tf.int32)\n","        out_type = tf.expand_dims(self.fc_type(self.model_cls(inputs)), axis = 1)\n","        out = tf.concat([ff, out_type], axis = 1)\n","        out = self.fc3(out)\n","        out = self.out1(out)\n","        out = self.flatten(out)\n","        out = self.out2(out)\n","        out = self.out3(out)\n","        return out\n","    \n","    def compile(self, loss, optimizer):\n","        super(QuestionAnalys, self).compile()\n","        self.optimizer = optimizer\n","        self.loss = loss\n","        self.train_loss = tf.keras.metrics.Mean()\n","        self.test_loss = tf.keras.metrics.Mean()\n","        \n","    @property\n","    def metrics(self):\n","        return [self.train_loss, self.test_loss]\n","\n","    def train_step(self, data):\n","        x1, x2, y = data\n","        with tf.GradientTape() as tape:\n","            predicts = self([x1, x2])\n","            l = self.loss(y, predicts)\n","        gradient = tape.gradient(l, self.trainable_variables)\n","        self.optimizer.apply_gradients(zip(gradient, self.trainable_variables))\n","        self.train_loss(l)\n","        return {'loss': self.train_loss.result()}\n","        \n","    def test_step(self, data):\n","        x1, x2, y = data\n","        predicts = self([x1, x2])\n","        l = self.loss(y, predicts)\n","        self.test_loss(l)\n","        return {'loss': self.test_loss.result()}"]},{"cell_type":"code","source":[""],"metadata":{"id":"vVfp0fCMcXZt"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"f-6HMosU_p-6"},"outputs":[],"source":["d_model = 256\n","num_heads = 6\n","model_analys = QuestionAnalys(d_model, maxlen, model_cls, num_heads)\n","model_analys([x1, x2]).shape"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oRJXZVazHqYq"},"outputs":[],"source":["model_analys.summary()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rUGhhnxIKjLp"},"outputs":[],"source":["optimizer = tf.keras.optimizers.Adam(1e-4)\n","loss = tf.keras.losses.MeanSquaredError(reduction='none')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nCHtlgL5b_0c"},"outputs":[],"source":["def loss_fn(y_true, y_pred):    \n","    loss_value = (y_true-y_pred)**2\n","    mask = tf.equal(y_true, tf.zeros_like(y_true))\n","    mask = tf.cast(tf.logical_not(mask), dtype = tf.float32)\n","    loss_value *= mask\n","    return tf.reduce_mean(loss_value)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"W8lFPfR0KshF"},"outputs":[],"source":["model_analys.compile(loss = loss_fn, optimizer=optimizer)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"E_4JvkjqQ6qw"},"outputs":[],"source":["hist = model_analys.fit(dataset_train, epochs=10, validation_data=dataset_test)"]},{"cell_type":"code","source":["import matplotlib.pyplot as plt"],"metadata":{"id":"X-PumMt_Zhnv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plt.plot(hist.history['loss'], label = 'loss')\n","plt.plot(hist.history['val_loss'], label = 'val loss')\n","plt.legend()\n","plt.show()"],"metadata":{"id":"pq8xrHINZm2O"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["checkpoint = tf.train.Checkpoint(\n","        model=model_analys)\n","ckpt_manager = tf.train.CheckpointManager(\n","    checkpoint, '/content/drive/MyDrive/Khóa luận tốt nghiệp/weight_of_word/chekcpoint3', max_to_keep=5)\n"],"metadata":{"id":"j65OXeK8ZvmK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["ckpt_manager.save()"],"metadata":{"id":"gc_AawgcnQvl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pwd"],"metadata":{"id":"tXkbisuHmeWB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"9azbw7vWmeTI"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aXjzXd4DQ-R6"},"outputs":[],"source":["# model_analys.save_weights('analys.weight')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"emQlp76yCANg"},"outputs":[],"source":["def predict(text):\n","    ids = tokenizer(text, max_len = maxlen, padding = True, truncation = True)['input_ids']\n","    ids = tf.convert_to_tensor([ids], dtype = tf.int32)\n","    pos = getTypeOfWord(text)\n","    pos = tf.expand_dims(tf.convert_to_tensor(pos, dtype = tf.int32), axis = 0)\n","    result = model_analys([ids, pos])\n","    return result"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":527,"status":"error","timestamp":1652924098701,"user":{"displayName":"Văn Nghiêm","userId":"03593021723549294443"},"user_tz":-420},"id":"p-DuGZaXV5n9","colab":{"base_uri":"https://localhost:8080/"},"outputId":"b7c5de6c-1ff2-4efa-eb53-fc0a70b5a0d2"},"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-6-80c66a35453b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'quy chế đào tạo theo hệ thống tín chỉ là gì'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvncore_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'%s ===> %.2f'\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'vncore_tokenize' is not defined"]}],"source":["q = 'quy chế đào tạo theo hệ thống tín chỉ là gì'\n","text = vncore_tokenize(q)\n","result = predict(text)\n","for i, j in enumerate(text.split()):\n","    print('%s ===> %.2f'%(j, result[0, i]))"]},{"cell_type":"code","source":["ViPosTagger.postagging(ViTokenizer.tokenize(q))"],"metadata":{"id":"cbDa5ALfNbN4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["q = 'không biết trách nhiệm của phòng công tác sinh viên là làm gì ạ em cám ơn'\n","text = vncore_tokenize(q)\n","result = predict(text)\n","for i, j in enumerate(text.split()):\n","    print('%s ===> %.2f'%(j, result[0, i]))"],"metadata":{"id":"vowO1UNBpjT6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["ViPosTagger.postagging(ViTokenizer.tokenize(q))"],"metadata":{"id":"_8MAw3RdNwja"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["q = 'chào admin em muốn hỏi niên giám là gì ạ'\n","text = vncore_tokenize(q)\n","result = predict(text)\n","for i, j in enumerate(text.split()):\n","    print('%s ===> %.2f'%(j, result[0, i]))"],"metadata":{"id":"OSMqpsYbpr3O"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["ViPosTagger.postagging(ViTokenizer.tokenize(q))"],"metadata":{"id":"GwRPaL2BN5M3"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TSc02jPrV8v4"},"outputs":[],"source":["model_analys.save_weights('weight_of_word/model.hdf5')"]},{"cell_type":"code","source":["class WeightModule(tf.Module):\n","    def __init__(self, model):\n","        self.model = model\n","    def __call__(self, x):\n","        predict = self.model(x)\n","        return predict"],"metadata":{"id":"SciHH5tBcHAC"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3SgFnS7MYDFD"},"outputs":[],"source":["weight = WeightModule(model_analys)"]},{"cell_type":"code","source":["weight([x1, x2]).shape"],"metadata":{"id":"w-wlRkqrYBxD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"yfqVqYhGn2X1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tf.saved_model.save(weight, 'weight_of_word/best_model')"],"metadata":{"id":"DPe8OzIQPJ0X"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["weight = tf.saved_model.load('weight_of_word/best_model')"],"metadata":{"id":"bx1vmTXJPZCz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"M48x809oRXjv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"NdR-mr4epO0n"},"execution_count":null,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"name":"weight-tf.ipynb","provenance":[],"machine_shape":"hm"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}